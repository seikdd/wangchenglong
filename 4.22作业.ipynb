{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 1, ',': 1, ';': 1, '?': 1, '!': 2, 'Dynamic': 1, 'type': 1, 'variables': 1, 'Python': 1, 'really': 1}\n"
     ]
    }
   ],
   "source": [
    "def authattr_worddict(doc):#输入字符串 返回字典\n",
    "\n",
    "    import re\n",
    "    import collections\n",
    "\n",
    "    list1=re.findall(r\"\\W\",doc)#正则匹配符号\n",
    "\n",
    "    list2=re.findall('[a-zA-Z0-9]+',doc)#正则匹配单词\n",
    "    list3=list1+list2\n",
    "    while ' ' in list3:\n",
    "        list3.remove(' ')\n",
    "#print(list3)\n",
    "    obj=collections.Counter(list3)\n",
    "    dict1=dict(obj)\n",
    "    #print(dict1)\n",
    "    return dict1\n",
    "\n",
    "str1=\"Dynamic-type variables,Python;really?!!\"\n",
    "print(authattr_worddict(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('j', 2), ('s', 2), ('i', 1), ('1', 1), ('2', 1), ('a', 1), ('h', 1), ('d', 1)]\n",
      "{'i': 5.5, '1': 5.5, '2': 5.5, 'j': 1.5, 's': 1.5, 'a': 5.5, 'h': 5.5, 'd': 5.5}\n",
      "[('h', 4), ('j', 2), ('x', 1), ('c', 1), ('k', 1), ('v', 1), ('i', 1), ('u', 1), ('2', 1)]\n",
      "{'j': 2.0, 'x': 6.0, 'c': 6.0, 'h': 1.0, 'k': 6.0, 'v': 6.0, 'i': 6.0, 'u': 6.0, '2': 6.0}\n",
      "[('1', 5.5), ('2', 5.5), ('a', 5.5), ('c', 'maxrank'), ('d', 5.5), ('h', 5.5), ('i', 5.5), ('j', 1.5), ('k', 'maxrank'), ('s', 1.5), ('u', 'maxrank'), ('v', 'maxrank'), ('x', 'maxrank')]\n",
      "[('1', 'maxrank'), ('2', 6.0), ('a', 'maxrank'), ('c', 6.0), ('d', 'maxrank'), ('h', 1.0), ('i', 6.0), ('j', 2.0), ('k', 6.0), ('s', 'maxrank'), ('u', 6.0), ('v', 6.0), ('x', 6.0)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "str1=\"i12jjsashd\"\n",
    "str2=\"jxchkvjhhiuh2\"\n",
    "list1=re.findall(r\"\\w\",str1)\n",
    "list2=re.findall(r\"\\w\",str2)\n",
    "\n",
    "count1=collections.Counter(list1)\n",
    "#print(count1.keys())#count1的key\n",
    "count2=collections.Counter(list2)\n",
    "#对第一个字典操作\n",
    "dict1=dict(count1)\n",
    "#print(dict1)#各个字符的计数\n",
    "print(sorted(count1.items(),key=lambda x:x[1],reverse=True))#排序后是列表[('a',2),('b',1)]\n",
    "vlist1=sorted(dict1.values(),reverse=True)#按值排序是列表\n",
    "#print(count2)\n",
    "#print(vlist1)\n",
    "count4=collections.Counter(vlist1)\n",
    "dict3=dict(count4)#出现频次的计数\n",
    "#print(dict3)\n",
    "dict5={}\n",
    "for key in dict3.keys():\n",
    "    dict5[key]=vlist1.index(key)+1+(dict3[key]-1)/2\n",
    "#print(dict5)#不同频率的rangking值\n",
    "for k2,v2 in dict1.items():\n",
    "    for k1,v1 in dict5.items():\n",
    "        if v2==k1:\n",
    "            dict1[k2]=v1\n",
    "print(dict1)\n",
    "#对第二个字典操作\n",
    "\n",
    "dict2=dict(count2)\n",
    "#print(dict2)#各个字符的计数\n",
    "print(sorted(count2.items(),key=lambda x:x[1],reverse=True))#排序后是列表[('a',2),('b',1)]\n",
    "vlist2=sorted(dict2.values(),reverse=True)#按值排序是列表\n",
    "#print(count2)\n",
    "#print(vlist2)\n",
    "count5=collections.Counter(vlist2)\n",
    "dict6=dict(count5)#出现频次的计数\n",
    "#print(dict6)\n",
    "dict7={}\n",
    "for key in dict6.keys():\n",
    "    dict7[key]=vlist2.index(key)+1+(dict6[key]-1)/2\n",
    "#print(dict7)#不同频率的rangking值\n",
    "for k2,v2 in dict2.items():\n",
    "    for k1,v1 in dict7.items():\n",
    "        if v2==k1:\n",
    "            dict2[k2]=v1\n",
    "print(dict2)\n",
    "for key in dict1:\n",
    "    if dict2.get(key):\n",
    "        pass\n",
    "    else:\n",
    "        dict2[key]=\"maxrank\"\n",
    "for key in dict2:\n",
    "    if dict1.get(key):\n",
    "        pass\n",
    "    else:\n",
    "        dict1[key]=\"maxrank\"\n",
    "#print(dict1)\n",
    "\n",
    "#print(dict2)\n",
    "print(sorted(dict1.items(),key=lambda x:x[0]))\n",
    "print(sorted(dict2.items(),key=lambda x:x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0\n"
     ]
    }
   ],
   "source": [
    "dict1={'a':10,'b':5,'c':5,'d':2,'e':2,'f':2,'g':1}\n",
    "dict2={'b':27,'h':22,'a':11,'i':11,'j':5}\n",
    "#dict1={'a':5000,'b':4000,'c':3000}\n",
    "#dict2={'a':5,'b':4,'c':3}\n",
    "def frequency2ranking(cdict):\n",
    "    import collections\n",
    "    vlist1=sorted(cdict.values(),reverse=True)#按值排序是列表\n",
    "\n",
    "    count4=collections.Counter(vlist1)\n",
    "    dict3=dict(count4)#出现频次的计数\n",
    "\n",
    "    dict5={}\n",
    "    for key in dict3.keys():\n",
    "        dict5[key]=vlist1.index(key)+1+(dict3[key]-1)/2\n",
    "\n",
    "    for k2,v2 in cdict.items():\n",
    "        for k1,v1 in dict5.items():\n",
    "            if v2==k1:\n",
    "                cdict[k2]=v1\n",
    "    return cdict\n",
    "\n",
    "#print(frequency2ranking(dict1))\n",
    "def authattr_oop(dictfrea1,dictfreq2,maxrank):\n",
    "    dictfrea1=frequency2ranking(dictfrea1)\n",
    "    dictfreq2=frequency2ranking(dictfreq2)\n",
    "    #print(dictfrea1)\n",
    "    #print(dictfreq2)\n",
    "    sum=0\n",
    "    for key in dictfrea1:\n",
    "        if dictfreq2.get(key):\n",
    "            pass\n",
    "        else:\n",
    "            dictfreq2[key]=maxrank\n",
    "    for key in dictfreq2:\n",
    "        if dictfrea1.get(key):\n",
    "            pass\n",
    "        else:\n",
    "            dictfrea1[key]=maxrank\n",
    "\n",
    "    for key in dictfrea1:\n",
    "        sum=sum+abs(dictfrea1[key]-dictfreq2[key])\n",
    "    print(sum)\n",
    "\n",
    "authattr_oop(dict1,dict2,10)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_lib模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authattr_worddict(doc):#输入字符串 返回字典\n",
    "\n",
    "    import re\n",
    "    import collections\n",
    "\n",
    "    list1=re.findall(r\"\\W\",doc)#正则匹配符号\n",
    "\n",
    "    list2=re.findall('[a-zA-Z0-9]+',doc)#正则匹配单词\n",
    "    list3=list1+list2\n",
    "    while ' ' in list3:\n",
    "        list3.remove(' ')\n",
    "#print(list3)\n",
    "    obj=collections.Counter(list3)\n",
    "    dict1=dict(obj)\n",
    "    #print(dict1)\n",
    "    return dict1\n",
    "\t\n",
    "def authattr_oop(dictfrea1,dictfreq2,maxrank):\n",
    "    dictfrea1=frequency2ranking(dictfrea1)\n",
    "    dictfreq2=frequency2ranking(dictfreq2)\n",
    "    #print(dictfrea1)\n",
    "    #print(dictfreq2)\n",
    "    sum=0\n",
    "    for key in dictfrea1:\n",
    "        if dictfreq2.get(key):\n",
    "            pass\n",
    "        else:\n",
    "            dictfreq2[key]=maxrank\n",
    "    for key in dictfreq2:\n",
    "        if dictfrea1.get(key):\n",
    "            pass\n",
    "        else:\n",
    "            dictfrea1[key]=maxrank\n",
    "\n",
    "    for key in dictfrea1:\n",
    "        sum=sum+abs(dictfrea1[key]-dictfreq2[key])\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hidden_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7b8c40ffcb08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mauthattr_authorpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Beatles'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hey Jude'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'The Fool on the Hill'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"A Hard Day's Night\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Yesterday\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Rolling Stones'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"(I Can't Get No)Satisfation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Ruby Tuesday'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Paint it Black'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Eleanor Rigby'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-7b8c40ffcb08>\u001b[0m in \u001b[0;36mauthattr_authorpred\u001b[1;34m(authordict, unknown, maxrank)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mauthattr_authorpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthordict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munknown\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mhidden_lib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauthattr_worddict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauthattr_oop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mauthordict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdict1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauthattr_worddict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthordict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hidden_lib'"
     ]
    }
   ],
   "source": [
    "def authattr_authorpred(authordict,unknown,maxrank):\n",
    "    from hidden_lib import authattr_worddict,authattr_oop\n",
    "    for key in authordict:\n",
    "        dict1=authattr_worddict(authordict[key])\n",
    "        print(dict1)\n",
    "\n",
    "\n",
    "authattr_authorpred({'Beatles':['Hey Jude','The Fool on the Hill',\"A Hard Day's Night\",\"Yesterday\"],'Rolling Stones':[\"(I Can't Get No)Satisfation\",'Ruby Tuesday','Paint it Black']},'Eleanor Rigby',15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读文件tox21.csv , 该文件为毒性分子数据库，其中O列smiles属性表示该分子式，B-M行表示该分子具有的12中化学属性，1表示有该属性，0表示无该属性，没有数字表示未知。\n",
    "根据tox21.csv生成一个新的.csv文件，该新文件包含12个sheet，每一个子文件表示一种属性。第一列为分子序号（与tox21.csv中的第A列序号一致），第二列为Mol_id(与tox21.csv中的N列内容一致)，第三列为分子式，第四列为是否具有该属性，若属性值未知，则该分子不包含在当前sheet中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'tox21.csv' does not exist: b'tox21.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bc4122d85fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_workbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcsv_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tox21.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'tox21.csv' does not exist: b'tox21.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "csv_data=pd.read_csv('tox21.csv')\n",
    "\n",
    "\n",
    "\n",
    "excelWriter=pd.ExcelWriter('out.xlsx',engine='openpyxl')#不用此函数excel的sheet会被覆盖掉\n",
    "\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet1',columns=[\"mol_id\",\"smiles\",\"NR-AR\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet2',columns=[\"mol_id\",\"smiles\",\"NR-AR-LBD\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet3',columns=[\"mol_id\",\"smiles\",\"NR-AhR\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet4',columns=[\"mol_id\",\"smiles\",\"NR-Aromatase\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet5',columns=[\"mol_id\",\"smiles\",\"NR-ER\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet6',columns=[\"mol_id\",\"smiles\",\"NR-ER-LBD\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet7',columns=[\"mol_id\",\"smiles\",\"NR-PPAR-gamma\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet8',columns=[\"mol_id\",\"smiles\",\"SR-ARE\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet9',columns=[\"mol_id\",\"smiles\",\"SR-ATAD5\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet10',columns=[\"mol_id\",\"smiles\",\"SR-HSE\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet11',columns=[\"mol_id\",\"smiles\",\"SR-MMP\"])\n",
    "csv_data.to_excel(excel_writer=excelWriter,sheet_name='sheet12',columns=[\"mol_id\",\"smiles\",\"SR-p53\"])\n",
    "excelWriter.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson1:SELECT queries 101 \n",
    "1.SELECT title FROM Movies;\n",
    "2.SELECT director FROM Movies;\n",
    "3.SELECT title,director FROM Movies;\n",
    "4.SELECT title,year FROM Movies;\n",
    "5.SELECT * FROM Movies;\n",
    "\n",
    "Lesson2:Queries with constraints (Pt. 1)\n",
    "1.SELECT * FROM Movies\n",
    "WHERE id=6;\n",
    "2.SELECT * FROM Movies\n",
    "WHERE Year BETWEEN 2000 AND 2010;\n",
    "3.SELECT * FROM Movies\n",
    "WHERE Year NOT BETWEEN 2000 AND 2010;\n",
    "4.SELECT Title,Year\n",
    "FROM Movies\n",
    "WHERE id IN(1,2,3,4,5);\n",
    "\n",
    "Lesson3:Queries with constraints (Pt. 2) \n",
    "1.SELECT * FROM Movies\n",
    "WHERE Title LIKE \"%Toy Story%\";\n",
    "2.SELECT * FROM Movies\n",
    "WHERE Director=\"John Lasseter\";\n",
    "3.SELECT * FROM Movies\n",
    "WHERE Director!=\"John Lasseter\";\n",
    "4.SELECT * FROM Movies\n",
    "WHERE TItle LIKE \"%WALL-%\";\n",
    "\n",
    "Lesson4:Filtering and sorting Query results\n",
    "1.SELECT DISTINCT Director FROM Movies\n",
    "ORDER BY Director ASC;\n",
    "2.SELECT Title,Year FROM Movies\n",
    "ORDER BY Year DESC\n",
    "LIMIT 4;\n",
    "3.SELECT * FROM Movies\n",
    "ORDER BY Title ASC\n",
    "LIMIT 5;\n",
    "4.SELECT * FROM Movies\n",
    "ORDER BY Title\n",
    "LIMIT 5 OFFSET 5;\n",
    "\n",
    "Review:Simple SELECT Queries \n",
    "1.SELECT City,Population FROM North_american_cities\n",
    "WHERE Country=\"Canada\";\n",
    "2.SELECT * FROM North_american_cities\n",
    "WHERE Country=\"United States\"\n",
    "ORDER BY Latitude DESC;\n",
    "3.SELECT * FROM North_american_cities\n",
    "WHERE Longitude<-87.629798\n",
    "ORDER BY Longitude ASC;\n",
    "4.SELECT City FROM North_american_cities\n",
    "WHERE Country=\"Mexico\"\n",
    "ORDER BY Population DESC\n",
    "LIMIT 2;\n",
    "5.SELECT City,Population FROM North_american_cities\n",
    "WHERE Country=\"United States\"\n",
    "ORDER BY Population DESC\n",
    "LIMIT 2 OFFSET 2;\n",
    "\n",
    "Lesson6:Multi-table queries with JOINs \n",
    "1.SELECT * FROM Movies\n",
    "INNER JOIN Boxoffice ON Movies.id=Boxoffice.Movie_id;\n",
    "2.SELECT Title,Domestic_sales,International_sales\n",
    "FROM Movies\n",
    "INNER JOIN Boxoffice ON Movies.id=Boxoffice.Movie_id\n",
    "WHERE International_sales>Domestic_sales;\n",
    "3.SELECT Title,Rating FROM Movies\n",
    "INNER JOIN Boxoffice ON MOvies.id=Boxoffice.Movie_id\n",
    "ORDER BY Rating DESC;\n",
    "\n",
    "Lesson7:OUTER JOINs\n",
    "1.SELECT DISTINCT Building FROM Employees;\n",
    "2.SELECT * FROM Buildings;\n",
    "3.SELECT DISTINCT Building_name,Role FROM Buildings\n",
    "LEFT JOIN Employees ON Employees.Building=Buildings.Building_name;\n",
    "\n",
    "Lesson8:A short note on NULLs \n",
    "1.SELECT * FROM Employees\n",
    "WHERE Building IS NULL;\n",
    "2.SELECT DISTINCT Building_name FROM Buildings\n",
    "LEFT JOIN Employees ON Employees.Building=Buildings.Building_name\n",
    "WHERE Role IS NULL;\n",
    "\n",
    "Lesson9:Queries with expressions \n",
    "1.SELECT title,(Domestic_sales+International_sales)/1000000 AS million\n",
    "FROM Movies\n",
    "INNER JOIN Boxoffice ON Boxoffice.Movie_id=Movies.id;\n",
    "2.SELECT Title,Rating*10 AS persent FROM Movies\n",
    "INNER JOIN Boxoffice on Movies.id=Boxoffice.Movie_id;\n",
    "3.SELECT * FROM MOvies WHERE YEAR%2=0;\n",
    "\n",
    "Lesson10:Queries with aggregates (Pt. 1) \n",
    "1.SELECT * FROM Employees\n",
    "ORDER BY Years_employed DESC\n",
    "LIMIT 1;\n",
    "2.SELECT *,AVG(Years_employed) AS average FROM Employees\n",
    "GROUP BY Role;\n",
    "3.SELECT *,SUM(Years_employed)\n",
    "FROM Employees\n",
    "GROUP BY Building;\n",
    "\n",
    "Lesson11:Queries with aggregates (Pt. 2) \n",
    "1.SELECT Role,COUNT(Name) AS number FROM Employees\n",
    "WHERE Role=\"Artist\";\n",
    "2.SELECT Role,COUNT(Name) AS number FROM Employees\n",
    "GROUP BY Role;\n",
    "3.SELECT Role,SUM(Years_employed) AS sumyear FROM Employees\n",
    "WHERE Role=\"Engineer\";\n",
    "\n",
    "Lesson12:Order of execution of a Query \n",
    "1.SELECT *,COUNT(Title) AS number FROM Movies\n",
    "GROUP BY Director;\n",
    "2.SELECT Director,SUM(Domestic_sales+International_sales) AS SUMM\n",
    "FROM Movies\n",
    "INNER JOIN Boxoffice ON Movies.id=Boxoffice.Movie_id\n",
    "GROUP BY Director;\n",
    "\n",
    "Lesson13:Inserting rows \n",
    "1.INSERT INTO Movies\n",
    "VALUES(4,\"TOy Story 4\",\"Pete Docter\",2019,130);\n",
    "2.INSERT INTO boxoffice VALUES (4, 8.7, 340000000, 270000000);\n",
    "\n",
    "Lesson14:Updating rows\n",
    "1.UPDATE movies\n",
    "SET director = \"John Lasseter\"\n",
    "WHERE id = 2;\n",
    "2.UPDATE Movies\n",
    "SET Year=1999\n",
    "WHERE id=3;\n",
    "UPDATE Movies\n",
    "SET Title=\"Toy Story 3\",Director=\"Lee Unkrich\"\n",
    "WHERE id=11;\n",
    "\n",
    "Lesson15:Deleting rows \n",
    "1.DELETE FROM Movies\n",
    "WHERE Year<2005;\n",
    "2.DELETE FROM MOvies\n",
    "WHERE Director=\"Andrew Stanton\";\n",
    "\n",
    "Lesson16:Creating tables\n",
    "CREATE TABLE Database(Name TEXT,Version FLOAT,Download_count INTEGER);\n",
    "\n",
    "Lesson17:Altering tables \n",
    "1.ALTER TABLE Movies\n",
    "ADD Aspect_ratio FLOAT;\n",
    "2.ALTER TABLE Movies\n",
    "ADD Language TEXT DEFAULT English;\n",
    "\n",
    "Lesson18:Dropping tables\n",
    "1.DROP TABLE IF EXISTS Movies;\n",
    "2.DROP TABLE IF EXISTS BoxOffice;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
